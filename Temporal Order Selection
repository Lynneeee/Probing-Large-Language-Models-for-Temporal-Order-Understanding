## For RoBERTa
import pandas as pd
import torch
from transformers import RobertaTokenizer, RobertaForMaskedLM

# Load the pre-trained model and tokenizer
model = RobertaForMaskedLM.from_pretrained('roberta-base')
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# Define a function to tokenize the input sentence
def tokenize(sentence):
    # Replace [MASK]  with <mask>
    sentence = sentence.replace('[MASK]', '<mask>')
    
    # Tokenize the sentence
    tokens = tokenizer.tokenize(sentence)
    
    return tokens

# Define a function to predict the top 5 candidates for the masked token
def predict_candidates(sentence):
    # Tokenize the sentence
    tokens = tokenize(sentence)
    
    # Find the index of the <mask> token
    mask_idx = tokens.index('<mask>')
    
    # Convert the tokens to input IDs
    input_ids = tokenizer.convert_tokens_to_ids(tokens)
    
    # Create a tensor for the input IDs
    input_tensor = torch.tensor([input_ids])
    
    # Predict the top 5 candidates for the masked token
    with torch.no_grad():
        outputs = model(input_tensor)
        predictions = outputs[0][0, mask_idx].topk(5)
    
    # Convert the predicted token IDs to tokens
    candidate_ids = predictions.indices.tolist()
    candidate_tokens = tokenizer.convert_ids_to_tokens(candidate_ids)
    
    return candidate_tokens

# Load the input data from an Excel file
df = pd.read_excel('/content/drive/MyDrive/thesis/0228_experimental data/five-predictions.XLSX')

# Predict the top 5 candidates for the masked token in each sentence
df['candidates'] = df['sentence'].apply(predict_candidates)

# Save the results to a new Excel file
df.to_excel('Roberta_output.xlsx', index=False)

# For DeBERTa
import pandas as pd
import torch
from transformers import DebertaTokenizer, DebertaForMaskedLM


# Load the pre-trained model and tokenizer
model = DebertaForMaskedLM.from_pretrained('microsoft/deberta-base')
tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')

# Define a function to tokenize the input sentence
def tokenize(sentence):
    # Replace <mask> with [MASK]
    sentence = sentence.replace('<mask>', '[MASK]')
    
    # Tokenize the sentence
    tokens = tokenizer.tokenize(sentence)
    
    return tokens

# Define a function to predict the top 5 candidates for the masked token
def predict_candidates(sentence):
    # Tokenize the sentence
    tokens = tokenize(sentence)
    
    # Find the index of the [MASK] token
    mask_idx = tokens.index('[MASK]')
    
    # Convert the tokens to input IDs
    input_ids = tokenizer.convert_tokens_to_ids(tokens)
    
    # Create a tensor for the input IDs
    input_tensor = torch.tensor([input_ids])
    
    # Predict the top 5 candidates for the masked token
    with torch.no_grad():
        outputs = model(input_tensor)
        predictions = outputs[0][0, mask_idx].topk(5)
    
    # Convert the predicted token IDs to tokens
    candidate_ids = predictions.indices.tolist()
    candidate_tokens = tokenizer.convert_ids_to_tokens(candidate_ids)
    
    return candidate_tokens

# Load the input data from an Excel file
df = pd.read_excel('/content/drive/MyDrive/thesis/0228_experimental data/five-predictions.XLSX')

## For BERT

import pandas as pd
import torch
from transformers import BertTokenizer, BertForMaskedLM

# Load the pre-trained model and tokenizer
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Define a function to tokenize the input sentence
def tokenize(sentence):
    # Replace <mask> with [MASK]
    sentence = sentence.replace('<mask>', '[MASK]')
    
    # Tokenize the sentence
    tokens = tokenizer.tokenize(sentence)
    
    return tokens

# Define a function to predict the top 5 candidates for the masked token
def predict_candidates(sentence):
    # Tokenize the sentence
    tokens = tokenize(sentence)
    
    # Find the index of the [MASK] token
    mask_idx = tokens.index('[MASK]')
    
    # Convert the tokens to input IDs
    input_ids = tokenizer.convert_tokens_to_ids(tokens)
    
    # Create a tensor for the input IDs
    input_tensor = torch.tensor([input_ids])
    
    # Predict the top 5 candidates for the masked token
    with torch.no_grad():
        outputs = model(input_tensor)
        predictions = outputs[0][0, mask_idx].topk(5)
    
    # Convert the predicted token IDs to tokens
    candidate_ids = predictions.indices.tolist()
    candidate_tokens = tokenizer.convert_ids_to_tokens(candidate_ids)
    
    return candidate_tokens

# Load the input data from an Excel file
df = pd.read_excel('/content/drive/MyDrive/thesis/0228_experimental data/five-predictions.XLSX')

# Predict the top 5 candidates for the masked token in each sentence
df['candidates'] = df['sentence'].apply(predict_candidates)

# Save the results to a new Excel file
df.to_excel('output.xlsx', index=False)


# Predict the top 5 candidates
df['candidates'] = df['sentence'].apply(predict_candidates)
# Save the results to a new Excel file
df.to_excel('Deberta_output.xlsx', index=False)

## For DeBERTa
import pandas as pd
import torch
from transformers import DebertaTokenizer, DebertaForMaskedLM


# Load the pre-trained model and tokenizer
model = DebertaForMaskedLM.from_pretrained('microsoft/deberta-base')
tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')

# Define a function to tokenize the input sentence
def tokenize(sentence):
    # Replace <mask> with [MASK]
    sentence = sentence.replace('<mask>', '[MASK]')
    
    # Tokenize the sentence
    tokens = tokenizer.tokenize(sentence)
    
    return tokens

# Define a function to predict the top 5 candidates for the masked token
def predict_candidates(sentence):
    # Tokenize the sentence
    tokens = tokenize(sentence)
    
    # Find the index of the [MASK] token
    mask_idx = tokens.index('[MASK]')
    
    # Convert the tokens to input IDs
    input_ids = tokenizer.convert_tokens_to_ids(tokens)
    
    # Create a tensor for the input IDs
    input_tensor = torch.tensor([input_ids])
    
    # Predict the top 5 candidates for the masked token
    with torch.no_grad():
        outputs = model(input_tensor)
        predictions = outputs[0][0, mask_idx].topk(5)
    
    # Convert the predicted token IDs to tokens
    candidate_ids = predictions.indices.tolist()
    candidate_tokens = tokenizer.convert_ids_to_tokens(candidate_ids)
    
    return candidate_tokens

# Load the input data from an Excel file
df = pd.read_excel('/content/drive/MyDrive/thesis/0228_experimental data/five-predictions.XLSX')

# Predict the top 5 candidates
df['candidates'] = df['sentence'].apply(predict_candidates)
# Save the results to a new Excel file
df.to_excel('Deberta_output.xlsx', index=False)
